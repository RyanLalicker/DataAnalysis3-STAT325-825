article{,
	title = {},
	author = {},
	year = {},
	date = {},
	journal = {},
	doi = {},
	url = {}
}

misc{,
  author    = {},
  title     = {},
  url       = {},
  year   = {}
}

@online{UNLBeef,
  author    = {Saner, Randy & Buseman, Brianna},
  title     = {How Many Pounds of Meat Can We Expect From A Beef Animal?},
  url       = {https://beef.unl.edu/beefwatch/2020/how-many-pounds-meat-can-we-expect-beef-animal},
  year   = {2024}
}

@online{USDA,
  author    = {USDA},
  title     = {5017-1: Calculating Dry Matter Intake from Pasture},
  url       = {https://www.ams.usda.gov/rules-regulations/organic/handbook/5017-1#:~:text=DMI%20is%20the%20level%20of,life%20and%20level%20of%20production.},
  year   = {}
}

@article{MEMON2023101382,
title = {A comparison of imputation methods for categorical data},
journal = {Informatics in Medicine Unlocked},
volume = {42},
pages = {101382},
year = {2023},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2023.101382},
url = {https://www.sciencedirect.com/science/article/pii/S2352914823002289},
author = {Shaheen MZ. Memon and Robert Wamala and Ignace H. Kabano},
keywords = {Imputation, Categorical variables, Precision score, Single imputation, Multiple imputation},
abstract = {Objectives
Missing data is commonplace in clinical databases, which are being increasingly used for research. Without giving any regard to missing data, results from analysis may become biased and unrepresentative. Clinical databases contain mainly categorical variables. This study aims to assess the methods used for imputation in categorical variables.
Materials and methods
We utilized data extracted from paper-based maternal health records from Kawempe National Referral Hospital, Uganda. We compared the following imputation methods for categorical data in an empirical analysis: Mode, K-Nearest Neighbors (KNN), Random Forest (RF), Sequential Hot-Deck (SHD), and Multiple Imputation by Chained Equations (MICE). The five imputation methods were first compared by accuracy of predicting the missing values. Next, the imputation methods were compared by predictive accuracy of the outcome variable in four classifiers. The consistency of performance of imputation methods across different levels of missing data (5%–50 %) was assessed by Kendall's W test.
Results
KNN imputation had the highest precision score at levels (5%–50 %) of MCAR missing data. At lower proportions of missing data (5 %, 10 %, 15 %, 20 %), RF imputation had the second-highest precision score. SHD imputation had the worst precision at all levels of missing data. In the prediction of the outcome, the methods performed differently at all proportions of missing data in the four classifiers. Even though KNN imputation was the best method in predicting the missing values, it did not consistently enhance the predictive accuracy of the classifiers at all levels of missing data. Our findings show that a high precision score of an imputation method does not translate into higher predictive accuracy in classifiers.
Conclusions
KNN imputation is the best method in predicting missing values in categorical variables. There is no universal best imputation method that yields the highest predictive accuracy at all proportions of missing data.}
}

@online{Freedman,
  author    = {William, Nkugwa Mark},
  title     = {How to Determine Bin Width for a Histogram ( R and Python)},
  url       = {https://nkugwamarkwilliam.medium.com/how-to-determine-bin-width-for-a-histogram-r-and-pyth-653598ab0d1c},
  year   = {2023}
}


@online{StAndrews,
  author    = {},
  title     = {Ordinal Regression},
  publisher = {University of St. Andrews},
  url       = {https://www.st-andrews.ac.uk/media/ceed/students/mathssupport/ordinal%20logistic%20regression.pdf},
  year   = {n.d.}
}

@online{ChiSquared,
  author    = {Bhalla, Deepanshu},
  title     = {How to Check Multicollinearity in Categorical Variables},
  publisher = {Listening Data},
  url       = {https://www.listendata.com/2015/04/detecting-multicollinearity-in-categorical-variables.html#:~:text=When%20dealing%20with%20nominal%20variables,suggest%20the%20presence%20of%20multicollinearity.},
  year   = {2017}
}

@article{Vigor,
	title = {Clinical Scoring Systems in the Newborn Calf: An Overview.},
	author = {Probo, Monica & Veronesi, Maria Cristina},
	year = {2022},
	date = {11-3-22},
	journal = {Animals (Basel)},
	doi = {doi: 10.3390/ani12213013},
	url = {https://pubmed.ncbi.nlm.nih.gov/36359137/}
}

@online{Ease,
  author    = {Heins, Brad & Pereira, Glenda},
  title     = {Monitoring calving traits to improve cow and calf health},
  publisher = {University of Minnesota Extension},
  url       = {https://extension.umn.edu/dairy-milking-cows/calving-traits#:~:text=Calving%20ease%20score%201%3A%20quick,score%204%3A%20used%20obstetrical%20chains.},
  year   = {2023}
}

@ONLINE{OLR,
  author = {},
  title = {Ordinal Logistic Regression | R Data Analysis Examples},
  year = {2011},
  publisher = {UCLA},
  url = {https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/}
}

@ONLINE{Multi,
  author = {Frost, Jim},
  title = {Multinomial Logistic Regression: Overview & Example},
  year = {2021},
  publisher = {Statistics By Jim},
  url = {https://statisticsbyjim.com/regression/multinomial-logistic-regression/}
}

@online{bino,
  author    = {Wiley, John},
  title     = {Binomial and Poisson Regression},
  url       = {https://www2.stat.duke.edu/courses/Fall21/sta521.001/post/week08-2/Applied_Linear_Regression_----_%28CHAPTER_12%29.pdf},
  year   = {2013}
}

@online{aic,
  author    = {Hyndman, Ron J.},
  title     = {Facts and fallacies of the AIC},
  url       = {https://robjhyndman.com/hyndsight/aic/},
  year   = {2013}
}

@ONLINE{ancova,
  author = {Frost, Jim},
  title = {ANCOVA: Uses, Assumptions & Example},
  year = {2023},
  publisher = {Statistics By Jim},
  url = {https://statisticsbyjim.com/anova/ancova/#comments}
}